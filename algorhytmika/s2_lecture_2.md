# Ллукция 2

## Однопроходные алгоритмы (прдолжение)

Рассмотрим задачу вычисления **квадрата среднеквадратического отклонения (СКО)** последовательности числовых данных.

Пусть дана числовая последовательность $a_1, a_2, ..., a_N$.

Требуется за один проход вычислить **квадрат среднеквадратического отклонения от среднего значения** этой последовательности.

Среднее значение последовательности - это есть среднее арифметическое всех ее членов:

$$ M = \frac{1}{N} \sum_{i=1}^{N} a_i $$

Квадрат **среднеквадратического отклонения** от среднего значения - это среднее арифметическое кваратов всех от всех отклонений от среднего значения:

$$ D = \frac{1}{N} \sum_{i=1}^{N} (a_i-M)^2 $$

Может показаться, на первый взгляд, что решить эту задачу за один проход не получится. В самом деле, казалось бы, что сначала, за один проход, будет необходимо вычислить величину $M$, а затем уже, за второй проход, вычислять $D$, непосредственно по имеющейся формуле. Однако оказывается, что для не индуктивной функции $D=D(a_1,...,a_N)$ имеется индуктивное расширение, которое позволит вычислить эту функцию всего за один проход. 

В самом деле
$$ D = \frac{1}{N} \sum_{i=1}^{N} (a_i-M)^2 = $$
$$ =\frac{1}{N} \sum_{i=1}^{N} (a_i^2-2a_iM+M^2) = $$
$$ =\frac{1}{N} \sum_{i=1}^{N} (a_i^2-2a_iM+M^2) = $$
$$ =\frac{1}{N} \sum_{i=1}^{N} a_i^2 - 2M\frac{1}{N}\sum_{i=1}^{N} a_i + M^2 = \frac{1}{N} \sum_{i=1}^{N} a_i^2 - M^2 $$
$$ D= \frac{1}{N} \sum_{i=1}^{N} a_i^2 - \Big( \frac{1}{N} \sum_{i=1}^{N} a_i \Big)^2 $$

Или, в окончательном виде,
$$ D= \frac{1}{N} S^{(2)}_N - \Big( \frac{1}{N} S^{(1)}_N \Big)^2 $$

где 

$$ S^{(2)}_N=\sum_{i=1}^{N} a_i^2 $$

$$ S^{(1)}_N=\sum_{i=1}^{N} a_i $$

Поскольку величины $N, S^{1}_N, S^{(2)}_N$, рассматриваемые как функции от последовательности, $A[1],A[2], ...,A[N]$ являются индуктивными, то кортеж их значений $\big(N, S^{1}_N, S^{(2)}_N \big)$ есть искомое индуктивное расширение функции $D$ (определенной на последовательностях).

Полученное индуктивное расширение важно с практической точки зрения тем, что, во первых, массив может быть очень длинным, и тогда экономия на его повторном проходе может оказаться весьма существенной. Во-вторых, это также дает возможность распространить данную задачу на случай, когда величина $N$ не считается фиксированной, а неограниченно возрастает: $N=1,2,3,...$ (рассматривается непрерывный поторк данных). В таком случае величины оценок и среднего значения $M=M_N$, и дисперсии $D=D_N$ будут считаться "текущими", т.е. зависящими от очередного $N$. Тогда наблюдая за значениями этих величин, можно будет, например, сделать вывод о "стационарности" потока данных, если эти величины почти не изменяются, или, в противном случае, - о отсутствии "стационарности" потока данных.

**Замечание.** В статистике величину $D$ называют оценкой **дисперсии** данных. А квадратный корень из этой величины называется оценкой **среднего квадратического отклонения** (СКО). СКО обычно обозначается символом $\sigma = \sqrt{(D)}$. Эта величина характеризует средний разброс отклонений значений последовательности от величины $M$.

В пакете `Statistics.jl` имеется функция `std`, вычисляющая оценку величины среднего квадратического отклонения.

Кроме того, в статистике величину $D$ часто определяют так же несколько по другой формуле:

$$ D = \frac{1}{N-1} \sum_{i=1}^{N} (a_i-M)^2 $$

Это так называемая **не смещенная оценка**, в отличие от **смещенной оценки** величины среднеквадратического отклонения, введенной ранее. Смысл этих понятий определяется в курсе математической статистики, которую  студенты-матаматики изучают на 3-ем курсе.

Но при больших $N$ отличия между этими двумя формулами будут малозначителыми.

## Быстрая сортировка Хоара

Ранее были расммотрены некоторые алгоритмы сортировки квадратичной сложности.

Существуют однако алгоритмы сортировки сложности $O(n*log(n))$. При этом можно доказать, что в общем случае, когда сортировка предполагает преремещения элементов (т.е. если речь не идет о сортировке подсчетом), то получить лучшую асимптотику не возможно.

Рассмотрим один из таких алгоритмов, который называется алгоритмом быстрой сортировки, или алгоритмом Хоара. Это один из самых эффективных и популярных алгоритмов сортировки.

В основе быстрой сортировки лежит следующая вспомогательная процедура частичной сортировки.

Пусть имеется массив $A$ и значение одного из его элементов, равное $b$. Требуется переставить элементы в массиве $A$ так, чтобы в нем сначала следовали все элементы, меньшие $b$, затем - все, равные $b$, а затем - все, большие $b$.

Причем алгоритмическая сложность этой процедуры должна оцениваться как $O(N)$, где $N=length(A)$, и дополнительные массивы.

Решим эту задачу с использованием метода инварианта цикла.

Напомним, что инвариантом цикла (с предусловием) называют утверждение (предикат), зависящее от фазовых переменных цикла (т.е. переменнных, которые могут изменяться в теле цикла), имеющее значение "истина" как до начала цикла, так и после любого числа его повторений.

Идея метода состоит в том, что для решения задачи сначала надо сформулировать подходящий инвариант цикла, а затем уже сконструировать соответствующий цикл.

Пусть величины $K, M, L$ (индексы массива $A$) такие, что

- $\forall i \in 1:K \ \ \ A[i]<b$
- $\forall i \in K+1:L \ \ \ A[i]==b$
- $\forall i \in M+1:N \ \ \ A[i]>b$

Эти три пункта, выполняемые одновременно, и будут составлять утверждение требуемого инварианта цикла. При этом в инварианте цикла ничего не утверждается про элементы массива для диапазона индексов $L+1:M$, этот диапазон индексов будет соответствовать еще не "обработаной" части массива. При чем в самом начале рассматриваемой процедуры частичной сортировки весь массив должен совпадать с этой "не обработанной" частью, а в конце процедуры "необработанная" часть должна будет получиться пустой.

Таким образом, искомая процедура частичной сортировки сводится к следующему циклу:

```julia
N = length(A)
K=0
L=0
M=N
#ИНВАРИАНТ: A[1:K] < b && A[K+1:L] == b && A[M+1:N] > b
while L < M 
    if A[L+1] == b
        L += 1
    elseif A[L+1] > b
        A[L+1], A[M] = A[M], A[L+1]
        M -= 1
    else # if A[L+1] < b
        L += 1; K += 1
        A[L], A[K] = A[K], A[L]
    end
end
```

Теперь с использованием этой вспомогательной процедуры массив может быть отсортирован рекурсивно следующим образом.

```julia
function quick_sort!(A)
    if isempty(A)
        return A
    end
    N = length(A)
    K, M = part_sort!(A, rand(1:N)) # - "базовый" элемент массива выбирается случайнам образом
    quick_sort!(A[1:K])
    quick_sort!(A[M:N])
    return A
end

function part_sort!(A, b)
    N = length(A)
    K=0
    L=0
    M=N
    #ИНВАРИАНТ: A[1:K] < b && A[K+1:L] == b && A[M+1:N] > b
    while L < M 
        if A[L+1] == b
            L += 1
        elseif A[L+1] > b
            A[L+1], A[M] = A[M], A[L+1]
            M -= 1
        else # if A[L+1] < b
            L += 1; K += 1
            A[L], A[K] = A[K], A[L]
        end
    end
    return K, M+1 
    # 1:K и M+1:N - эти диапазоны индексов определяют ещё не 
    # отсортированные части массива A
end
```

Оценить сложность алгоритма быстрой сортироваки quick_sort! можно следующим образом. Поскольку каждый раз перед процедурой частичной сортировки part_sort! "базовый" элемент $b$ массива A выбирается случайным образом, то можно считать, что **в среднем** размеры его частей $A[1:K] и A[M+1:N]$, подлежащих дальнейшей сортировке, но уже по отдельности, будут получаться приблизительно равными.

Для упрощения анализа условимся считать их равными $N/2$. Тогда всю процедуру сортировки можно будет представить двоичным деревом высоты $log_2(N)$ (каждый узел этого дерева фиксирует факт разделения рекурсивно сортируемой части массива на два подмассива равной длины). При этом корню этого дерева соответствует $N$ операций сранения, и каждому его последующему ярусу тоже сооттветствуют те же $N$ операций сравнения.

Таким образом, потребуется всего $N\log_2N$ операций, т.е. оценка сложности алгоритма сортировки в среднестатистическом смысле может быть выражена как $O(N\log(N))$.

## Порядковые статистики, алгоритм быстрого вычисления порядковых статистик

Пусть имеется числовой массив $А$. Его $k$-ой **порядковой статистикой** называется значение $k$-го элемента этого массива, которое получилось бы после реализации процедуры сортировки массива $A$.

Однако для вычисления $k$-ой порядковой статистики ($k$ считается фиксированным) вовсе не обязательно сортировать массив. Существует алгоритм вычисления этой величины, имеющий сложность всего $O(N)$, т.е. быстрый алгоритм.

В самом деле, если индекс $k$ задан, то требуемую процедуру вычисдения $k$-ой порядковой статистики легко получить из рассмотренного выше алгоритма Хоара быстрой сортировки. В самом деле, вычисление $k$-ой порядковой статистики от процедуры сортировки Хоара будет отличаться лишь тем, что для последующей после частичной сортирвки массива дальнейшая обработка массива должна бутет производиться лишь только над одной из двух полученных его частей, а именно, той из них, диапазон индексов которой включает заданное $k$.

В результате, первая частичная сортирвка даст $O(N)$ операций сравнения, вторая - $O(N/2)$, третья - $O(N/4)$ и т.д., что в сумме составит всего $O(N)$ операций.

**Замечание.** Минимальное и максмальное значения массива явяются его 1-ой и $N$-ой порядковыми статистиками, соответственно. Однако, хотя сложность их вычисления также оценивается как $O(N)$, их вычисление обычным способом потребует вдвое меньше сравнений (не говоря уже о том, что оно вовсе не потребует престановок элементов массива, что существенно более затратно по сравнению с просто опереацией сравнения).

## Медиана массива

Если длина $N$ массива нечетная, то его медианой называется порядковая статистика с индексом $(N-1)/2$. В случае же четной длины массива его медианой можно считать среднее арифметическое двух порядковых статистик с индексами $N/2-1$ и $N/2+1$.

Поэтому алгоритм вычисления медианы массива также может основываться на быстром алгоритме вычисления порядковых статистик.

Медиана массива, наряду со средним (средним арифметическим) значением массива является важной статистической характеристикой содержащихся в нем данных. Например, эта характеристика более адекватно оценивает уровень доходов большинства граждан, нежели простое среднее арифметическое значение уровня доходов всех граждан.

## Алгоритмы сортировки Шелла и "расчесыванием"

Рассмотренный здесь алгорим быстрой сортирвки Хоара является одним из наиболее эффективных и популярных алгоритмов сортировки.

Однако рассматривавшиеся ранее алгоритмы сортировки методом "пузырька" и вставками, имеющими квадрадратичную асимптотическую оценку сложности, также могут быть существенно улучшены. При этом эффективность таких улучшенных алгоритмов может оказаться сравнимой с эффективностью алгоритма Хоара.

Основная проблема алгоритма пузырьковой сортировки и алгоритма вставками состоит не в том, что они требуют $O(N^2)$ операций сравнения, а в том, что для их реализации может потребоваться столько же перестановок соседних элементов массива, т.к. перестановки элементов значительно более затратная процедура по сравнению с операцией сравнения.

Поэтому, если каким либо образом удастся уменьшить количество таких перстановок, то это очень существенно сократит время всей сортировки.

Зададимся вопросом, когда при выполнении указанных сортировок требуется наибольшее количество перестановок. Ответ, очевидно, будет следующим, - если в начале исходного массива было много "больших" чисел, потому, что тогда эти числа придется многократно обменивать с соедними до тех пор, пока они не окажутся на своих местах ближе к концу массива.

Число таких обменов можно попытаться уменьшить, если вначале сравнивать и, при нобходимости, менять местами не соседние  элементы масива, а только отстоящие друг от друга на значительно большее расстояние. Затем это расстояние надо будет уменьшить и повторить ту же процедуру частичной сортировки массива с самого его начала. Постепенно меньшать расстояние между сравниваемыми элементами, и повторять процедуру частичной сортировки надо до тех пор, пока это расстояние не станет равным 1, т.е. пока, наконец, не начнут сравниваться соседние элементы массива.

Это общая идея повышения эффетивности рассмотренных ранее алгоритмов сортирвки квадратичной сложности. При этом в обоих случаях первоначальное расстояние между сравниваемыми элементами массива перется равным длине массива (т.е. в начале сравниваются только первый и последний элементы массива).

В случае классической сортировки Шелла, базирующейся на сортировке вставками, в классическом её вариантк, это расстояние каждый раз уменьшается вдвое (приблизительно). При достижении расстояния между элементами равным 1 и после последнего выполнеиия сотрировки вставками уже всего массива, массив, очевидно, окажется отсортированным. Однако количество потребовавшихся при этом перестановок соседних элементов массива будет уже относительно небольшим, так все самые большие элементы ранее уже оказались перемещенными ближе к концу массива.

В случае же сортировки "расчесыванием", базирующейся на "пузырьковой" сортировке, это расстояние каждый раз уменьшается путем деления его на некоторый коэффициент, больший 1 (импирически установлено, что этот коэффициент лучше всего взять равным приблизительно 1.247). Однако в отличие от сортировки Шелла, при сортировке  "расчесыванием", когда дело уже дойдет до сравнения соседних элементов массиива, остается в точности не известно, сколько же еще раз надо пройти массив, сравнивая соседние элементы, чтобы он оказался полностью отсорированным. Но это число будет уже совсем небольшим (обычно остается сделать всего несколько проходов).

Подробно сортировка Шелла и сортировка "прочесыванием" будут рассмотрены на практическом занятии.